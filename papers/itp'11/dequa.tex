\documentclass{article}
\usepackage[english]{babel}
\usepackage{amssymb, amsmath}
\usepackage{fullpage, url}
\usepackage{biblatex, csquotes}

\include{definitions}

\title{Using reflection to solve some differential equations}
\author{Guillaume Allais\\
Junior Laboratory \coqtail{}\\
ENS Lyon - France}

\bibliography{biblio}

\begin{document}
\maketitle{}

\begin{abstract}
On top of \coqtail{}'s libraries that provide a formalization of power series
comes a small development that aims at simplifying proofs that given power
series are solutions of specific differential equations. The use of reflection
allows to prove general facts about differential equations which can then be
used to simplify the proofs thanks to an \texttt{Ltac} machinery that performs
the tedious conversions.

\textbf{Files:} All the implementation mentioned in this paper are
available for download via \coqtail{}'s svn repository\footnote{see
\url{http://sourceforge.net/projects/coqtail/develop}
and in particular \texttt{src/Reals/Dequa\_*.v}.}.
\end{abstract}

\section{Definitions}

Even if the \dequa{} library relies heavily on \coqtail{}'s \Rpser{}
definitions and developments, one does not need to know much about these in
order to read this presentation. We recall here the basic definitions that
are used afterwards.

The sequence of coefficients of the power series defining the constant function
$\lambda\_. C$ is given by the $\C{}$ function. The infiniteness of its radius
of convergence is obviously part of the library.

$$\C{}(C) := n \mapsto \left\lbrace
\begin{array}{ll}
C & \text{if } n = 0\\
0 & \text{otherwise}
\end{array}
\right.$$

The coefficients of the formal $k^{th}$ derivative of a power series defined by
$A_n$ is given by the $\Dn{}$ function. The link between the formal derivative
and the actual one is also stated and proved in the available libraries.

$$\Dn{}(A_n, k) := n \mapsto \frac{(n + k)!}{n!}A_{n+k}$$

Given a sequence of coefficients $A_n$ and a proof $\rho$ that the convergence
radius of $\sum_n A_n x^n$ is infinite, the sum of the power series is given by
the function $\Psum{}$:

$$\Psum{}(A_n, \rho) := x \mapsto \sum_{k=0}^{+ \infty} A_k x^k$$

\section{Reflection of differential equations}

In order to represent the differential equations, we need two components: a
datatype which can describe the structure of a differential equation and a
semantics which, given a structure and a context, yields a formula in
\Prop{} stating that the functions in the context are solutions of the
described differential equation.

\subsection{Reification}

A differential equation $\forall x. e1(x) = e2(x)$ is represented as a pair
$(E_1, E_2)$ of side equations (usually written $E_1 :=: E_2$) where $E_1$ (resp.
$E_2$) represents $e_1$ (resp. $e_2$). A side equation is either a constant,
a function's $n^{th}$ derivative or a linear combination of side equations. We
use the following datatype to describe such side equations:


\begin{verbatim}
Inductive side_equa : Set :=
    | cst  : forall (r : R), side_equa
    | scal : forall (r : R) (s : side_equa), side_equa
    | y    : forall (p : nat) (k : nat), side_equa
    | opp  : forall (s1 : side_equa), side_equa
    | plus : forall (s1 s2 : side_equa), side_equa.
\end{verbatim}

This representation can be rather easily extended with new data constructors if
the corresponding theory has been formalized in \coq{}~\cite{coq}: adding the multiplication
by a scalar (which was not present in our first toy example) was a matter of less
than twenty minutes as soon as all the appropriate lemmas where available in
\Rpser{}\footnote{We hope to modify the data structure to be able to talk about
the product of two functions in a near future.}.

\subsection{Interpretation}

Unlike other procedure using reflection, \dequa{} provides various semantics for
these side equations: the straightforward one (\`a la Tarski) but also a semantics
that yields equations on sequences over $\R$.

These semantics are defined in two steps: given an environment,
$\Interp{}_{\mathbb{K}}$ translates side equations and
$\left[\left| \_ \right|\right]_{\mathbb{K}}$ uses it to interpret equations.
For the sake of simplicity, instead of enforcing the well-formedness of the
environment by a dependent type (e.g. a dependent vector),
the defined functions are partial and use the $\option{}$ monad.

$\mathtt{Rseq\_infty}$ is the subset of sequences over $\R$ such that the
corresponding power series has an infinite convergence radius (a dependent
pair in \coq{}).

\subsubsection{Equations on power series}

Our first concern is to defined the semantics \`a la Tarski that translates
differential equations as differential equations on sums of power series.
$\IR : \se \rightarrow \alist{} \mathtt{Rseq\_infty} \rightarrow \option{}
(\texttt{R} \rightarrow \texttt{R})$ translates constants as the constant function,
variables as derivatives of sums of power series of the corresponding $\Rseq$
available in the environment and linear combinations as linear combinations of
functions\footnote{See \ref{interpR} for technical details.}.

$\left[\left| \_ \right|\right]_{\R} : \de \rightarrow \alist{}
\mathtt{Rseq\_infty} \rightarrow \Prop$ is the corresponding interpretation
function: given the interpretation of two side equations, it states that the
obtained functions are extensionally equal.

\subsubsection{Equations on sequences over $\R$}

The second semantics translates differential equations as equations on
sequences over $\R$.  The function $\IN :
\se \rightarrow \alist{} \Rseq \rightarrow \option{} \Rseq$ translates
constants as $\C{}$, variables as $\Dn{}$ and linear combinations as linear
combinations of sequences\footnote{See \ref{interpN} for technical details.}.

$\left[\left| \_ \right|\right]_{\N} : \de \rightarrow \alist{}
\mathtt{Rseq\_infty} \rightarrow \Prop$ is the corresponding interpretation
function:  given the interpretation of two side equations, it states that the
obtained sequences are extensionally equal.

\subsection{Relation between these semantics}

The relation between these semantics comes from two simple facts: given two
sequences of coefficients extensionally equal, the corresponding power
series are equal and sums of power series are compatible with sum (the sum
of the power series is the power series of the sum), opposite, etc.
Therefore it is sufficient to prove that some coefficients are solutions of
the equation on sequences in order to prove that the sums of the corresponding
power series are solutions of the differential equations.


We assume the existence of a projection $proj_1$ that given a sequence $A_n$ of
type $\mathtt{Rseq\_infty}$ forgets the proof that $\sum_n A_n x^n$ has an
infinite convergence radius: $proj_1 A_n$ is of type $\Rseq{}$ (in \coq{} we
simply use $\mathtt{projT1}$).

\begin{theorem}Our main result states that given a context
$\rho$ of type $\alist{} \mathtt{Rseq\_infty}$:
$$\left[\left| e_1 :=: e_2 \right|\right]_\N (map ~ proj_1 ~ \rho) \Rightarrow
\left[\left| e_1 :=: e_2 \right|\right]_\R \rho$$

ie. we can prove that some functions (sums of power series) are solutions
of a given differential equation by proving results on their coefficients.
\end{theorem}

\begin{proof} The proof uses the following intermediate lemma:
$$\IN(e, map ~ proj_1 ~ \rho) = \Some U_n \wedge \IR(e, \rho) = \Some f$$
$$\Rightarrow \exists \rho. \forall x. f(x) = \Psum{}(U_n, \rho)(x)$$
which is proved by induction on $e$.
\end{proof}

\subsection{Going back to actual problems}

We just proved a theorem that gives us the opportunity to show that some functions
are solutions of differential equations just by looking at the coefficients of
their power series. This theorem is however quite useless without tactics on top
of it: one has to feed it with a $\de$ and an environment and it will output a
statement that has a very specific shape (given by the function $\left[\left|
 \_ \right|\right]_\R$) which is highly unlikely to match the current goal.

Because quoting differential equations and normalizing goals is tedious enough
for us to want to avoid doing it by hand, we wrote a couple of tactics that do
all the hard work for us. We have two main tactics corresponding to the two
main steps: quoting and normalizing.

\subsubsection{Quoting differential equations}

There are different ways of quoting the same differential equation depending on
whether you care about the proofs (that the radius of convergence is infinite,
that the power series can be derived, etc.) used or not and whether a constant
has to be a value or not.

We chose not to care about the proofs because none of the values computed by the
operations we consider are influenced by the proof used. Therefore
$\forall x. \Psum(an, r1)(x) = \Psum(an, r2)(x)$ will be represented as
$y(0,0) = y(0,0)$.

We also chose to allow constants to be complex expressions and not only values
in order to guarantee that the $\de$ will be as simple as possible.
We will therefore quote $\forall x. \Psum(an,  r1)(x) = 5 * (4 + 3)$ as
$y(0,0) = cst (5 * (4 + 3))$ rather than $y(0,0) = scal (5, (plus ((cst(4)), (cst(3))))$.

Given this choices the design of the tactic machinery is quite simple. We only
need to code:
\begin{itemize}
\item{$\mathtt{isconst}(s, x)$} that checks whether the expression $s$ is a constant
  with respect to $x$.

\item{$\mathtt{add\_variables(a_n, pr, env)}$} that adds $(a_n, pr)$ to the
  environment $env$ if there is no $(a_n, pr')$ already declared\footnote{where
  $pr$, $pr'$ are proofs that $a_n$ has an infinite convergence radius.}
  and returns an updated environment together with an integer representing $a_n$.

\item{$\mathtt{quote\_side\_equa(env, s, x)}$} that proceeds as follow:
  \subitem if $\mathtt{isconst}(s, x)$ then return $cst(s)$ and $env$
  \subitem else if the head constructor of $s$ is in $\left\lbrace -\_; \_+\_;
   \_-\_\right\rbrace$ then translate it to the appropriate $\se$ constructor
   (either $\mathtt{opp}$, $\mathtt{plus}$, $\mathtt{minus}$ and recursively
   quote the subexpressions while propagating the environment
  \subitem else if the head constructor of $s$ is $\_*\_$ then one of the
   subexpressions has to be a constant\footnote{we do not yet support non linear
   differential equations}; recursively quote the non-constant equation and
   output a $\mathtt{scal}$ together with the updated environment
  \subitem else if the expression is a sum or a derivative, add the corresponding
   sequence to the environment \textit{via} $\mathtt{add\_variables}$ that returns
   a $p \in \N$ and an updated environment and propagate the environment while
   outputing $y(p,k)$ where $k$ is such that the quoted expression is a $k^{th}$
   derivative.
\end{itemize}

\subsubsection{Normalizing goals}

The aim of the goal's normalization procedure is to modify the goal so that it
matches $\left[\left| E \right|\right]_\R \rho$ where $E$ and $\rho$ are
respectively the representation of the goal and the corresponding environment.
If we look closely at $\left[\left| \_ \right|\right]_\R$'s code, we can remark
that:
\begin{itemize}
\item $\mathtt{scal(k,s)}$ is always translated as $k * s'$ where $s'$ is the
  translation of $s$. We must therefore use the commutativity of the addition
  to put all the constant expression on the left side of their non-constant
  counterpart

\item $\mathtt{y(p,k)}$ is always translated as $\nthder(\Psum(\rho_p),k)$ and we must
  therefore:
  \subitem replace every occurence of $\Psum (\mathtt{fst} (\rho_p), pr')$ where
  $pr' \neq \mathtt{snd}(\rho_p)$ by $\Psum(\rho_p)$
  \subitem replace every occurence of $\Psum (\rho_p)(x)$ with $\nthder(\Psum(\rho_p),0)$
  \subitem replace every occurence of $\mathtt{derive}(\Psum (\rho_p))(x)$ with
  $\nthder(\Psum(\rho_p),1)$
\end{itemize}

This normalization is possible thanks to the lemma stating that all these
substitutions are sound. Here are two examples of such lemmas:

\begin{verbatim}
Lemma nth_derive_sum_PI_O : forall an (r1 r2 : infinite_cv_radius an),
  nth_derive (sum an r1) (D_infty_Rpser an r1 O) == sum an r2.

Lemma nth_derive_sum_PI_1 : forall an (r1 r2 : infinite_cv_radius an)
  (pr : derivable (sum an r2)),
  nth_derive (sum an r1) (D_infty_Rpser an r1 1) == derive (sum an r2) pr.
\end{verbatim}

\section{Applications}

Thanks to this theorem, one can prove in less than 20 lines that the
exponential is a solution of the equation $y^{(n+1)} = y^{(n)}$. Instead
of having to deal with limits and derivatives, one just has to prove that:

$$\forall k \in \N, \frac{((n+1) + k)!}{k!} * \frac{1}{((n+1)+k)!}
= \frac{(n+k)!}{k!} * \frac{1}{(n+k)!}$$

Without much more work, one can also prove that cosine and sine are
both solutions of $y^{(2)} = - y$.

\printbibliography{}

\include{appendix}

\end{document}
