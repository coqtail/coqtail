\documentclass{article}
\usepackage[english]{babel}
\usepackage{amssymb, amsmath}
\usepackage{fullpage, url}
\usepackage{biblatex, csquotes}

\include{definitions}

\title{Using reflection to solve some differential equations}
\author{Guillaume Allais\\
Junior Laboratory \coqtail{}\\
ENS Lyon - France}

\bibliography{biblio}

\begin{document}
\maketitle{}

\begin{abstract}
On top of \coqtail{}'s libraries that provide a formalization of power series, we
added a small development that aims at simplifying proofs that given power
series are solutions of specific differential equations. The use of reflection
allows to prove general facts about differential equations which can then be
used to simplify the proofs thanks to an \texttt{Ltac} machinery that performs
the tedious conversions.

\textbf{Files:} All the implementation mentioned in this paper are
available for download via \coqtail{}'s svn repository\footnote{see
\url{http://sourceforge.net/projects/coqtail/develop}
and in particular \texttt{src/Reals/Dequa\_*.v}.}.
\end{abstract}

\section{Definitions}

Even if the \dequa{} library relies heavily on \coqtail{}'s \Rpser{}
definitions and developments, one does not need to know much about these in
order to read this presentation. We recall here the basic definitions that
are used afterwards.

The sequence of coefficients of the power series defining the constant function
$\lambda\_. C$ is given by the $\C{}$ function. The infiniteness of its radius
of convergence is obviously part of the library.

$$\C{}(C) := n \mapsto \left\lbrace
\begin{array}{ll}
C & \text{if } n = 0\\
0 & \text{otherwise}
\end{array}
\right.$$

The coefficients of the formal $k^{th}$ derivative of a power series defined by
$A_n$ is given by the $\Dn{}$ function. The link between the formal derivative
and the actual one is also stated and proved in the available libraries.

$$\Dn{}(A_n, k) := n \mapsto \frac{(n + k)!}{n!}A_{n+k}$$

Given a sequence of coefficients $A_n$ and a proof $\rho$ that the convergence
radius of $\sum_n A_n x^n$ is infinite, the sum of the power series is given by
the function $\Psum{}$:

$$\Psum{}(A_n, \rho) := x \mapsto \sum_{k=0}^{+ \infty} A_k x^k$$

\section{Reflection of differential equations}

In order to represent the differential equations, we need two components: a
datatype which can describe the structure of a differential equation and a
semantics which, given a structure and a context, yields a formula in
\Prop{} stating that the functions in the context are solutions of the
described differential equation.

\subsection{Reification}

A differential equation $\forall x. e1(x) = e2(x)$ is represented as a pair
$(E_1, E_2)$ of side equations (usually written $E_1 :=: E_2$) where $E_1$ (resp.
$E_2$) represents $e_1$ (resp. $e_2$). A side equation is either a constant,
a function's $n^{th}$ derivative or a linear combination of side equations. We
use the following datatype to describe such side equations:


\begin{verbatim}
Inductive side_equa : Set :=
    | cst  : forall (r : R), side_equa
    | scal : forall (r : R) (s : side_equa), side_equa
    | y    : forall (p : nat) (k : nat), side_equa
    | opp  : forall (s1 : side_equa), side_equa
    | plus : forall (s1 s2 : side_equa), side_equa.
\end{verbatim}

This representation can be rather easily extended with new data constructors
when the corresponding theory has been formalized in \coq{}~\cite{coq}: adding
the multiplication by a scalar (which was not present in our first toy example)
was a matter of less than twenty minutes as soon as all the appropriate lemmas
were available in \Rpser{}\footnote{We hope to modify the data structure to be
able to talk about the product of two functions in a near future.}.

\subsection{Interpretation}

Unlike other procedures using reflection, \dequa{} provides various semantics for
these side equations: the straightforward one (\`a la Tarski) but also a semantics
that yields equations on sequences of coefficients over $\R$.

These semantics ($\K$ is either $\R$ or $\N$) are defined in two steps:
given an environment, $\IK$ translates side equations and
$\left[\left| \_ \right|\right]_{\K}$ uses it to interpret equations. For the
sake of simplicity, instead of enforcing the well-formedness of the environment
by a dependent type (e.g. a dependent vector), the defined functions are partial
and use the $\option$ monad.

$\mathtt{Rseq\_infty}$ is the subset of sequences over $\R$ such that the
corresponding power series has an infinite convergence radius (a dependent
pair in \coq{}).

\subsubsection{Equations on power series}

Our first concern is to define the semantics \`a la Tarski that translates
$\de$ as differential equations on sums of power series.
$\IR : \se \rightarrow \alist{} \mathtt{Rseq\_infty} \rightarrow \option{}
(\texttt{R} \rightarrow \texttt{R})$ translates constants as the constant function,
variables as derivatives of sums of power series of the corresponding $\Rseq$
available in the environment and linear combinations as linear combinations of
functions\footnote{See \ref{interpR} for technical details.}.

$\left[\left| \_ \right|\right]_{\R} : \de \rightarrow \alist{}
\mathtt{Rseq\_infty} \rightarrow \Prop$ is the corresponding interpretation
function: given the interpretation of two side equations, it states that the
obtained functions are extensionally equal.

\subsubsection{Equations on sequences over $\R$}

The second semantics translates $\de$ as equations on
sequences over $\R$.  The function $\IN :
\se \rightarrow \alist{} \Rseq \rightarrow \option{} \Rseq$ translates
constants as $\C{}$, variables as $\Dn{}$ and linear combinations as linear
combinations of sequences\footnote{See \ref{interpN} for technical details.}.

$\left[\left| \_ \right|\right]_{\N} : \de \rightarrow \alist{}
\mathtt{Rseq} \rightarrow \Prop$ is the corresponding interpretation
function:  given the interpretation of two side equations, it states that the
obtained sequences are extensionally equal.

\subsection{Relation between these semantics}

The relation between these semantics comes from two simple facts: given two
sequences of coefficients extensionally equal, the corresponding power
series are equal and sums of power series are compatible with sum (the sum
of the power series is the power series of the sum), opposite, etc.
Therefore it is sufficient to prove that some coefficients are solutions of
the equation on sequences in order to prove that the sums of the corresponding
power series are solutions of the differential equations.

We define $proj_1$ the projection which, given a sequence $A_n$ of type
$\mathtt{Rseq\_infty}$, forgets the proof that $\sum_n A_n x^n$ has an
infinite convergence radius: $proj_1 A_n$ is of type $\Rseq{}$ (in \coq{} we
simply use $\mathtt{projT1}$).

\begin{theorem} Our main result states that given a context
$\rho$ of type $\alist{} \mathtt{Rseq\_infty}$:
$$\left[\left| e_1 :=: e_2 \right|\right]_\N (map ~ proj_1 ~ \rho) \Rightarrow
\left[\left| e_1 :=: e_2 \right|\right]_\R \rho$$

ie. we can prove that some functions (sums of power series) are solutions
of a given differential equation by proving results on their coefficients.
\end{theorem}

\begin{proof} The proof uses the following intermediate lemma:
$$\IN(e, map ~ proj_1 ~ \rho) = \Some U_n \wedge \IR(e, \rho) = \Some f$$
$$\Rightarrow \exists \rho. \forall x. f(x) = \Psum{}(U_n, \rho)(x)$$
which is proved by induction on $e$.
\end{proof}

\subsection{Going back to actual problems}

We just proved a theorem that gives us the opportunity to show that some functions
are solutions of differential equations just by looking at the coefficients of
their power series. This theorem is however quite useless without tactics on top
of it: one has to feed it with a $\de$ and an environment and it will output a
statement that has a very specific shape (given by the function $\left[\left|
 \_ \right|\right]_\R$) which is highly unlikely to match the current goal.

Because quoting differential equations and normalizing goals is tedious enough
for us to want to avoid doing it by hand, we wrote a couple of tactics that do
all the hard work for us. We have two main kinds of tactics corresponding to the
two main steps: quoting and normalizing.

\subsubsection{Quoting differential equations}

There are different ways of quoting the same differential equation depending on
whether you care about the proofs (that the radius of convergence is infinite,
that you can take the derivative of the power series, etc.) used or not and
whether a constant has to be a value or not.

We chose not to care about the proofs because none of the values computed by the
operations we consider are influenced by the proof used. Therefore
$\forall x. \Psum(a_n, pr_1)(x) = \Psum(a_n, pr_2)(x)$ will be represented as
$\y(0,0) = \y(0,0)$.

We also chose to allow constants to be complex expressions and not only values
in order to guarantee that the generated $\de$ will be as simple as possible.
We will therefore quote $\forall x. \Psum(a_n, pr_1)(x) = 5 * (4 + 3)$ as
$\y(0,0) = \cst (5 * (4 + 3))$ rather than $\y(0,0) = \scal (5, (\plus ((\cst(4)),
(\cst(3))))$.

Given this choices, the tactic machinery should look like:
\begin{itemize}
\item{$\mathtt{isconst}(s, x)$} that checks whether the expression $s$ is a constant
  with respect to $x$.

\item{$\mathtt{add\_variables(a_n, pr, env)}$} that adds $(a_n, pr)$ to the
  environment $env$ if there is no $(a_n, pr')$ already declared\footnote{where
  $pr$, $pr'$ are proofs that $a_n$ has an infinite convergence radius.}
  and returns an updated environment together with an integer representing
  $(a_n, pr)$.

\item{$\mathtt{quote\_side\_equa(env, s, x)}$} that proceeds as follow:
  \subitem if $\mathtt{isconst}(s, x)$ then return $\cst(s)$ and $env$
  \subitem else if the head constructor of $s$ is in $\left\lbrace -\_; \_+\_;
   \_-\_\right\rbrace$ then translate it to the appropriate $\se$ constructor
   (either $\opp$, $\plus$ or $\minus$) and recursively quote the subexpressions
    while propagating the environment
  \subitem else if the head constructor of $s$ is $\_*\_$ then one of the
   subexpressions has to be a constant\footnote{We do not support non linear
   differential equations yet.}; recursively quote the non-constant equation and
   output a $\scal$ together with the updated environment
  \subitem else if the expression is a sum or a derivative, add the corresponding
   sequence to the environment \textit{via} $\mathtt{add\_variables}$ that returns
   a $p \in \N$ and an updated environment and propagate the environment while
   outputting $\y(p,k)$ where $k$ is such that the quoted expression is a $k^{th}$
   derivative.
\end{itemize}

\subsubsection{Normalizing goals}

The aim of the goal's normalization procedure is to modify the goal so that it
matches $\left[\left| E \right|\right]_\R \rho$ where $E$ and $\rho$ are
respectively the representation of the goal and the corresponding environment.
If we look closely at $\left[\left| \_ \right|\right]_\R$'s code, we can remark
that:
\begin{itemize}
\item $\scal(k,s)$ is always translated as $k * s'$ where $s'$ is the
  translation of $s$. We must therefore use the commutativity of the addition
  to put all the constant expression on the left side of their non-constant
  counterpart

\item $\y(p,k)$ is always translated as $\nthder(\Psum(\rho_p),k)$ and we must
  therefore:
  \subitem replace every occurrence of $\Psum (\mathtt{fst} (\rho_p), pr')$ where
  $pr' \neq \mathtt{snd}(\rho_p)$ by $\Psum(\rho_p)$
  \subitem replace every occurrence of $\Psum (\rho_p)(x)$ with $\nthder(\Psum(\rho_p),0)$
  \subitem replace every occurrence of $\mathtt{derive}(\Psum (\rho_p))(x)$ with
  $\nthder(\Psum(\rho_p),1)$
\end{itemize}

This normalization is possible thanks to lemmas stating that all these
substitutions are sound. Here are two examples of such lemmas:

\begin{verbatim}
  Lemma nth_derive_sum_PI_O : forall an (r1 r2 : infinite_cv_radius an),
    nth_derive (sum an r1) (D_infty_Rpser an r1 O) == sum an r2.

  Lemma nth_derive_sum_PI_1 : forall an (r1 r2 : infinite_cv_radius an)
    (pr : derivable (sum an r2)),
    nth_derive (sum an r1) (D_infty_Rpser an r1 1) == derive (sum an r2) pr.
\end{verbatim}

\subsubsection{End-user tactics}

The end-user of our libraries is obviously not supposed to call these tactics
that deal with environment, sub-expressions, variable names, etc. We provide two
friendlier tactics (namely \texttt{solve\_diff\_equa\_on} and
\texttt{solve\_diff\_equa}) that hide away all these details:

\begin{itemize}
\item $\mathtt{solve\_diff\_equa\_on}(x)$ inspects the goal. If the goal is an
equality statement,
  \subitem It quotes both sides by using $\mathtt{quote\_side\_equa}$ with an
  appropriate environment (the empty one first and the one returned by the first
  call then) and $x$; as a result, it gets two $\se$ $E_1$ and $E_2$ and an
  environment $env$
  \subitem It then normalizes the equation by calling $\mathtt{normalize}$ on
  both sides with $env$
  \subitem The last step is to assume $\SemN$ (the user will be asked to prove
  this fact later) \textit{via} \texttt{cut} and use it together with our
  theorem to prov the current goal
\item $\mathtt{solve\_diff\_equa\_on}$  is simply
  $\mathtt{solve\_diff\_equa\_on}(x)$  where $x$ is a fresh variable introduced
  by the tactic.
\end{itemize}

\section{Applications}

Thanks to this theorem and these tactics, one can prove in less than 20
lines\footnote{See \texttt{Dequa\_examples} for more details on these proofs.}
that the exponential is a solution of the equation $y^{(n+1)} = y^{(n)}$. Instead
of having to deal with limits and derivatives, one just has to prove that:

$$\forall k \in \N, \frac{((n+1) + k)!}{k!} * \frac{1}{((n+1)+k)!}
= \frac{(n+k)!}{k!} * \frac{1}{(n+k)!}$$

Without much more work, one can also prove that cosine and sine are
both solutions of $y^{(2)} = - y$.

\printbibliography{}

\include{appendix}

\end{document}
